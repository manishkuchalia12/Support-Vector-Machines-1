{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfa4755d-9b0e-4035-af89-7e64e95c52f6",
   "metadata": {},
   "source": [
    "Q1. What is the mathematical formula for a linear SVM?\n",
    "The mathematical formula for a linear Support Vector Machine (SVM) involves finding a hyperplane that separates the data into different classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97dc4889-f48d-4528-b313-8e674c6a5dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "\n",
    "# Sample data\n",
    "X = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])\n",
    "y = np.array([1, 1, -1, -1])\n",
    "\n",
    "# Create a linear SVM model\n",
    "svm_model = SVC(kernel='linear')\n",
    "\n",
    "# Fit the model to the data\n",
    "svm_model.fit(X, y)\n",
    "\n",
    "# Get the learned weights and bias\n",
    "w = svm_model.coef_\n",
    "b = svm_model.intercept_\n",
    "\n",
    "print(\"Weight vector (w):\", w)\n",
    "print(\"Bias term (b):\", b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06033085-231c-41d3-b053-828d23933ad9",
   "metadata": {},
   "source": [
    "Q2. What is the objective function of a linear SVM?\n",
    "Ans:-The objective function of a linear Support Vector Machine (SVM) is to find the parameters (weights and bias) that define a hyperplane, maximizing the margin between different classes in the feature space while minimizing classification errors. The formulation involves a trade-off between achieving a large margin and tolerating some misclassifications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23439880-2aa3-48e2-a28e-4f6056f51874",
   "metadata": {},
   "source": [
    "Q3. What is the kernel trick in SVM?\n",
    "Ans:-The kernel trick in Support Vector Machines (SVM) is a technique that allows SVMs to handle non-linear decision boundaries by implicitly mapping input data into a higher-dimensional feature space without explicitly computing the transformed feature vectors. This is achieved through the use of a kernel function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef2e87b-416b-4aa7-8ab8-c23096d1b0d2",
   "metadata": {},
   "source": [
    "Q4. What is the role of support vectors in SVM Explain with example\n",
    "Ans:-In a Support Vector Machine (SVM), support vectors are the data points that lie closest to the decision boundary (hyperplane) and have a significant influence on the positioning of the decision boundary. These are the critical data points that \"support\" the definition of the decision boundary and play a crucial role in determining the optimal hyperplane.\r\n",
    "\r\n",
    "Support vectors are the only data points that contribute to the computation of the decision function. The positions of these support vectors relative to the decision boundary influence the margin, and they determine the orientation and location of the hyperplane. All other data points, which are not support vectors, do not affect the decision boundary and are essentially ignored.\r\n",
    "\r\n",
    "Let's illustrate the role of support vectors with a simple example in Python using the Scikit-learn library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d534540c-d7f8-4fa3-93f1-15ac80a3ee35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm\n",
    "\n",
    "# Generate synthetic data with two classes\n",
    "np.random.seed(42)\n",
    "X = np.random.randn(20, 2)\n",
    "y = np.concatenate([-np.ones(10), np.ones(10)])\n",
    "\n",
    "# Create an SVM model with a linear kernel\n",
    "model = svm.SVC(kernel='linear', C=1)\n",
    "model.fit(X, y)\n",
    "\n",
    "# Plot the data points and decision boundary\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired, marker='o', s=100)\n",
    "\n",
    "# Plot the decision boundary\n",
    "ax = plt.gca()\n",
    "xlim = ax.get_xlim()\n",
    "ylim = ax.get_ylim()\n",
    "\n",
    "# Create grid to evaluate model\n",
    "xx, yy = np.meshgrid(np.linspace(xlim[0], xlim[1], 50),\n",
    "                     np.linspace(ylim[0], ylim[1], 50))\n",
    "Z = model.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "# Plot decision boundary and margins\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.contour(xx, yy, Z, colors='k', levels=[-1, 0, 1], alpha=0.5, linestyles=['--', '-', '--'])\n",
    "\n",
    "# Plot support vectors\n",
    "plt.scatter(model.support_vectors_[:, 0], model.support_vectors_[:, 1],\n",
    "            s=300, linewidth=1, facecolors='none', edgecolors='k')\n",
    "\n",
    "plt.title('SVM Decision Boundary with Support Vectors')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f987c7d-de71-4dc9-b494-c17696dee99c",
   "metadata": {},
   "source": [
    "Q5. Illustrate with examples and graphs of Hyperplane, Marginal plane, Soft margin and Hard margin in\n",
    "SVM?\n",
    "Ans:-Sure, let's illustrate the concepts of Hyperplane, Marginal plane, Soft margin, and Hard margin in SVM using examples and graphs. We'll use Python with the Scikit-learn library for this demonstration.\r\n",
    "\r\n",
    "1. Hyperplane:\r\n",
    "The hyperplane is the decision boundary that separates data points of different classes. For a binary classification problem, the hyperplane is a (D-1)-dimensional plane in the feature space, where D is the number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d15818e-2936-4922-8462-83e3261c18f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm\n",
    "\n",
    "# Generate synthetic data\n",
    "np.random.seed(42)\n",
    "X = np.random.randn(50, 2)\n",
    "y = np.concatenate([-np.ones(25), np.ones(25)])\n",
    "\n",
    "# Create an SVM model with a linear kernel\n",
    "model = svm.SVC(kernel='linear', C=1)\n",
    "model.fit(X, y)\n",
    "\n",
    "# Plot the data points and decision boundary\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired, marker='o', s=100)\n",
    "ax = plt.gca()\n",
    "xlim = ax.get_xlim()\n",
    "ylim = ax.get_ylim()\n",
    "\n",
    "# Create grid to evaluate model\n",
    "xx, yy = np.meshgrid(np.linspace(xlim[0], xlim[1], 50),\n",
    "                     np.linspace(ylim[0], ylim[1], 50))\n",
    "Z = model.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "# Plot decision boundary\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.contour(xx, yy, Z, colors='k', levels=[0], alpha=0.5, linestyles=['-'])\n",
    "\n",
    "plt.title('SVM Hyperplane')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96271ebf-4155-4c1f-a3b4-6a76ea817b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Marginal Plane:\n",
    "# Plot the data points, decision boundary, and margins\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired, marker='o', s=100)\n",
    "\n",
    "# Plot decision boundary and margins\n",
    "plt.contour(xx, yy, Z, colors='k', levels=[-1, 0, 1], alpha=0.5, linestyles=['--', '-', '--'])\n",
    "\n",
    "# Plot support vectors\n",
    "plt.scatter(model.support_vectors_[:, 0], model.support_vectors_[:, 1],\n",
    "            s=300, linewidth=1, facecolors='none', edgecolors='k')\n",
    "\n",
    "plt.title('SVM Margins with Support Vectors')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
